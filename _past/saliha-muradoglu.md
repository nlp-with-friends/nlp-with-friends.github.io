---
permalink: /speakers/saliha-muradoglu
title: "Do transformer models do phonology like a linguist?"
speaker: Saliha Muradoǧlu
author: Saliha Muradoǧlu
date: Sept 22, 2021
time: 0:00 UTC
timeconverter: "https://www.timeanddate.com/worldclock/converter.html?iso=20210922T000000&p1=1440&p2=224&p3=179&p4=136&p5=676&p6=33&p7=152"
read_time: true
layout: single
author_profile: true
---

<a href="https://lolmythesis.com/" class="one-line">Testing the linguistic understanding of transformer generalizations</a>

Neural sequence-to-sequence models have been very successful at tasks in phonology and morphology that seemingly require a capacity for intricate linguistic generalizations. Despite their success, the opaque nature of neural models renders the task of analysing and evaluating the generalisations produced difficult. To compare the generalisations generated by these models with those of linguistic tradition, we experiment with phonological processes on a constructed language. We establish that the models are capable of learning 29 different phonological processes with varying degrees of complexity. We explore whether the models generalise over linguistic categories such as vowels and consonants, whether they learn a representation of internal word structures and finally, more complex phonological processes such as rule ordering.

<hr>

**Saliha** is a Linguistics PhD student at the Australian National University. Her research focuses on using neural networks in aid of language documentation, with particular focus on low-resource settings.

#### Presentation Materials
<i class="fas fa-fw fa-video"></i> [Talk Video](https://www.youtube.com/watch?v=xcCSLy-wuO0&list=PL0zsOCvKa2iEqmPV6WGhjuP-tsrUy102C){:target="_blank"}  
